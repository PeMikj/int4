{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-21T21:29:05.608878Z",
     "iopub.status.busy": "2025-11-21T21:29:05.608639Z",
     "iopub.status.idle": "2025-11-21T21:36:32.626396Z",
     "shell.execute_reply": "2025-11-21T21:36:32.625323Z",
     "shell.execute_reply.started": "2025-11-21T21:29:05.608850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cu121 -qq\n",
    "!pip install triton==2.1.0 -qq\n",
    "!pip install bitsandbytes==0.41.1 -qq\n",
    "!pip install unsloth -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:36:32.630101Z",
     "iopub.status.busy": "2025-11-21T21:36:32.629584Z",
     "iopub.status.idle": "2025-11-21T21:37:19.369831Z",
     "shell.execute_reply": "2025-11-21T21:37:19.369052Z",
     "shell.execute_reply.started": "2025-11-21T21:36:32.630051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 21:36:43.311744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763761003.778758      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763761003.943961      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "TORCH: 2.9.0+cu128\n",
      "TRITON: 3.5.0\n",
      "BNB: 0.48.2\n",
      "UNSLOTH: 2025.11.3\n"
     ]
    }
   ],
   "source": [
    "import torch, triton, bitsandbytes as bnb, unsloth\n",
    "print(\"TORCH:\", torch.__version__)\n",
    "print(\"TRITON:\", triton.__version__)\n",
    "print(\"BNB:\", bnb.__version__)\n",
    "print(\"UNSLOTH:\", unsloth.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:39:23.334989Z",
     "iopub.status.busy": "2025-11-21T21:39:23.334639Z",
     "iopub.status.idle": "2025-11-21T21:39:23.362039Z",
     "shell.execute_reply": "2025-11-21T21:39:23.361261Z",
     "shell.execute_reply.started": "2025-11-21T21:39:23.334964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _quantize_rowwise_int4(\n",
    "    w_ptr, packed_ptr, scale_ptr, zero_ptr,\n",
    "    rows, cols,\n",
    "    stride_wm, stride_wn,\n",
    "    stride_pm, stride_pn,\n",
    "    BLOCK_SIZE: tl.constexpr = 64,\n",
    "):\n",
    "    row_id = tl.program_id(0)\n",
    "    if row_id >= rows:\n",
    "        return\n",
    "\n",
    "    row_w = w_ptr + row_id * stride_wm\n",
    "\n",
    "    row_min = 1e9\n",
    "    row_max = -1e9\n",
    "    for start in range(0, tl.cdiv(cols, BLOCK_SIZE)):\n",
    "        offs = start * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offs < cols\n",
    "        vals = tl.load(row_w + offs * stride_wn, mask=mask, other=0.0).to(tl.float32)\n",
    "        row_min = tl.minimum(row_min, tl.min(tl.where(mask, vals, 1e9), axis=0))\n",
    "        row_max = tl.maximum(row_max, tl.max(tl.where(mask, vals, -1e9), axis=0))\n",
    "\n",
    "    range_val = row_max - row_min\n",
    "    range_safe = tl.maximum(range_val, 1e-6)\n",
    "    scale = range_safe / 15.0\n",
    "    zero_point = tl.extra.cuda.libdevice.rint(-row_min / scale)\n",
    "    zero_point = tl.minimum(tl.maximum(zero_point, 0.0), 15.0)\n",
    "\n",
    "    tl.store(scale_ptr + row_id, scale.to(tl.float32))\n",
    "    tl.store(zero_ptr + row_id, zero_point.to(tl.float32))\n",
    "\n",
    "    packed_cols = tl.cdiv(cols, 2)\n",
    "\n",
    "    for start in range(0, tl.cdiv(packed_cols, BLOCK_SIZE)):\n",
    "        offs = start * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offs < packed_cols\n",
    "\n",
    "        even_idx = offs * 2\n",
    "        odd_idx  = even_idx + 1\n",
    "\n",
    "        even_mask = mask & (even_idx < cols)\n",
    "        odd_mask  = mask & (odd_idx < cols)\n",
    "\n",
    "        even_vals = tl.load(row_w + even_idx * stride_wn, mask=even_mask, other=0.0).to(tl.float32)\n",
    "        odd_vals  = tl.load(row_w + odd_idx  * stride_wn, mask=odd_mask,  other=0.0).to(tl.float32)\n",
    "\n",
    "        even_q = tl.extra.cuda.libdevice.rint(even_vals / scale + zero_point)\n",
    "        odd_q  = tl.extra.cuda.libdevice.rint(odd_vals  / scale + zero_point)\n",
    "        even_q = tl.minimum(tl.maximum(even_q, 0.0), 15.0)\n",
    "        odd_q  = tl.minimum(tl.maximum(odd_q, 0.0), 15.0)\n",
    "\n",
    "        even_u = even_q.to(tl.uint8)\n",
    "        odd_u  = odd_q.to(tl.uint8)\n",
    "        packed_val = even_u | (odd_u << 4)\n",
    "\n",
    "        col_p = packed_ptr + offs * stride_pm\n",
    "        tl.store(col_p + row_id * stride_pn, packed_val, mask=mask)\n",
    "\n",
    "\n",
    "def quantize_rowwise_int4(weight):\n",
    "    weight = weight.contiguous().to(torch.float16)\n",
    "    rows, cols = weight.shape\n",
    "    packed_cols = (cols + 1) // 2\n",
    "\n",
    "    packed = torch.empty((packed_cols, rows), dtype=torch.uint8, device=weight.device)\n",
    "    scales = torch.empty(rows, dtype=torch.float32, device=weight.device)\n",
    "    zeros = torch.empty(rows, dtype=torch.float32, device=weight.device)\n",
    "\n",
    "    grid = (rows,)\n",
    "    _quantize_rowwise_int4[grid](\n",
    "        weight, packed, scales, zeros,\n",
    "        rows, cols,\n",
    "        weight.stride(0), weight.stride(1),\n",
    "        packed.stride(0), packed.stride(1),\n",
    "    )\n",
    "    return packed, scales, zeros\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _matmul_fp16_int4(\n",
    "    a_ptr, b_ptr, scale_ptr, zero_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bm, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_M: tl.constexpr = 64,\n",
    "    BLOCK_N: tl.constexpr = 64,\n",
    "    BLOCK_K: tl.constexpr = 64,\n",
    "    NUM_WARPS: tl.constexpr = 4,\n",
    "    NUM_STAGES: tl.constexpr = 3,\n",
    "):\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "\n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "\n",
    "    mask_m = offs_m < M\n",
    "    mask_n = offs_n < N\n",
    "\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    packed_cols = tl.cdiv(K, 2)\n",
    "\n",
    "    scales = tl.load(scale_ptr + offs_n, mask=mask_n, other=1.0).to(tl.float32)\n",
    "    zeros = tl.load(zero_ptr + offs_n, mask=mask_n, other=0.0).to(tl.float32)\n",
    "\n",
    "    for k0 in range(0, K, BLOCK_K):\n",
    "        k = k0 + tl.arange(0, BLOCK_K)\n",
    "        mask_k = k < K\n",
    "\n",
    "        a_tile = tl.load(\n",
    "            a_ptr + offs_m[:, None] * stride_am + k[None, :] * stride_ak,\n",
    "            mask=mask_m[:, None] & mask_k[None, :],\n",
    "            other=0.0\n",
    "        ).to(tl.float32)\n",
    "\n",
    "        pack_idx = k // 2\n",
    "        mask_pack = pack_idx < packed_cols\n",
    "\n",
    "        w_pack = tl.load(\n",
    "            b_ptr + pack_idx[None, :] * stride_bm + offs_n[:, None] * stride_bn,\n",
    "            mask=mask_n[:, None] & mask_pack[None, :],\n",
    "            other=0\n",
    "        ).to(tl.uint8)\n",
    "\n",
    "        low  = (w_pack & 0x0F).to(tl.float32)\n",
    "        high = ((w_pack >> 4) & 0x0F).to(tl.float32)\n",
    "        w_tile = tl.where((k % 2 == 1)[None, :], high, low)\n",
    "\n",
    "        w_tile = (w_tile - zeros[:, None]) * scales[:, None]\n",
    "\n",
    "        acc += tl.dot(a_tile, tl.trans(w_tile))\n",
    "\n",
    "    tl.store(\n",
    "        c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn,\n",
    "        acc.to(tl.float16),\n",
    "        mask=mask_m[:, None] & mask_n[None, :],\n",
    "    )\n",
    "\n",
    "\n",
    "def matmul_fp16_int4(a, b_packed, scales, zeros):\n",
    "    a = a.contiguous().to(torch.float16)\n",
    "    M, K = a.shape\n",
    "    packed_K, N = b_packed.shape\n",
    "    assert packed_K >= (K + 1) // 2\n",
    "\n",
    "    c = torch.empty((M, N), dtype=torch.float16, device=a.device)\n",
    "\n",
    "    grid = (triton.cdiv(M, 64), triton.cdiv(N, 64))\n",
    "\n",
    "    _matmul_fp16_int4[grid](\n",
    "        a, b_packed, scales, zeros,\n",
    "        c, M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b_packed.stride(0), b_packed.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "    )\n",
    "    return c\n",
    "\n",
    "\n",
    "class QuantLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        packed_cols = (in_features + 1) // 2\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight_packed\",\n",
    "            torch.zeros((packed_cols, out_features), dtype=torch.uint8),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"weight_scale\",\n",
    "            torch.ones(out_features, dtype=torch.float32),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"weight_zero\",\n",
    "            torch.zeros(out_features, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features, dtype=torch.float16))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def quantize_from_fp16(self, weight: torch.Tensor, bias: torch.Tensor | None = None):\n",
    "        weight = weight.reshape(self.out_features, self.in_features)\n",
    "\n",
    "        packed, scales, zeros = quantize_rowwise_int4(weight)\n",
    "\n",
    "        self.weight_packed.copy_(packed)\n",
    "        self.weight_scale.copy_(scales)\n",
    "        self.weight_zero.copy_(zeros)\n",
    "\n",
    "        if self.bias is not None and bias is not None:\n",
    "            self.bias.copy_(bias.to(torch.float16))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig = x.shape[:-1]\n",
    "        x = x.reshape(-1, self.in_features)\n",
    "\n",
    "        out = matmul_fp16_int4(\n",
    "            x,\n",
    "            self.weight_packed,\n",
    "            self.weight_scale,\n",
    "            self.weight_zero\n",
    "        )\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out.reshape(*orig, self.out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:39:23.363435Z",
     "iopub.status.busy": "2025-11-21T21:39:23.363195Z",
     "iopub.status.idle": "2025-11-21T21:39:23.386269Z",
     "shell.execute_reply": "2025-11-21T21:39:23.385735Z",
     "shell.execute_reply.started": "2025-11-21T21:39:23.363418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TARGETS = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "           \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
    "\n",
    "\n",
    "def replace_linears_with_quant(module):\n",
    "    for name, child in list(module.named_children()):\n",
    "\n",
    "        if name in TARGETS and isinstance(child, torch.nn.Linear):\n",
    "\n",
    "            out_f, in_f = child.weight.shape\n",
    "\n",
    "            q = QuantLinear(in_f, out_f, bias=(child.bias is not None)).cuda()\n",
    "            q.quantize_from_fp16(child.weight, child.bias)\n",
    "\n",
    "            setattr(module, name, q)\n",
    "            continue\n",
    "\n",
    "        replace_linears_with_quant(child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:39:23.438659Z",
     "iopub.status.busy": "2025-11-21T21:39:23.438419Z",
     "iopub.status.idle": "2025-11-21T21:39:27.422720Z",
     "shell.execute_reply": "2025-11-21T21:39:27.421932Z",
     "shell.execute_reply.started": "2025-11-21T21:39:23.438615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "replace_linears_with_quant(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:39:27.424395Z",
     "iopub.status.busy": "2025-11-21T21:39:27.424118Z",
     "iopub.status.idle": "2025-11-21T21:39:32.274615Z",
     "shell.execute_reply": "2025-11-21T21:39:32.273788Z",
     "shell.execute_reply.started": "2025-11-21T21:39:27.424368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Who is Albert Einstein? Albert Einstein was a German-born physicist who is widely regarded as one of the most intelligent and insightful people in history. He was a key figure in the development of the field of physics and mathematics, particularly\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is Albert Einstein?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:39:32.275739Z",
     "iopub.status.busy": "2025-11-21T21:39:32.275504Z",
     "iopub.status.idle": "2025-11-21T21:39:34.986413Z",
     "shell.execute_reply": "2025-11-21T21:39:34.985572Z",
     "shell.execute_reply.started": "2025-11-21T21:39:32.275723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_int4_model/tokenizer_config.json',\n",
       " 'my_int4_model/special_tokens_map.json',\n",
       " 'my_int4_model/chat_template.jinja',\n",
       " 'my_int4_model/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"my_int4_model\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"my_int4_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:39:34.987605Z",
     "iopub.status.busy": "2025-11-21T21:39:34.987329Z",
     "iopub.status.idle": "2025-11-21T21:39:34.995393Z",
     "shell.execute_reply": "2025-11-21T21:39:34.994600Z",
     "shell.execute_reply.started": "2025-11-21T21:39:34.987582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "  import torch\n",
    "  import time\n",
    "\n",
    "  def benchmark_matmuls():\n",
    "      device = \"cuda\"\n",
    "\n",
    "      print(\"Warmup...\")\n",
    "      Xw = torch.randn(64, 4096, dtype=torch.float16, device=device)\n",
    "      Ww = torch.randn(4096, 4096, dtype=torch.float16, device=device)\n",
    "      Wp, Sc, Zr = quantize_rowwise_int4(Ww)\n",
    "      _ = matmul_fp16_int4(Xw, Wp, Sc, Zr)\n",
    "      torch.cuda.synchronize()\n",
    "      print(\"Warmup done.\\n\")\n",
    "\n",
    "      tests = [\n",
    "          (128, 4096, 4096),\n",
    "          (512, 4096, 4096),\n",
    "          (2048, 4096, 4096),\n",
    "          (128, 4096, 14336),\n",
    "          (512, 4096, 14336),\n",
    "          (2048, 4096, 14336),\n",
    "      ]\n",
    "\n",
    "      for B, K, N in tests:\n",
    "          print(f\"=== X: {B}Ã—{K}, W: {N}Ã—{K} ===\")\n",
    "\n",
    "          X = torch.randn(B, K, dtype=torch.float16, device=device)\n",
    "          W = torch.randn(N, K, dtype=torch.float16, device=device)\n",
    "\n",
    "          torch.cuda.synchronize()\n",
    "          t0 = time.time()\n",
    "          Y_fp16 = X @ W.T\n",
    "          torch.cuda.synchronize()\n",
    "          t_fp16 = (time.time() - t0) * 1000\n",
    "\n",
    "          W_packed, scales, zeros = quantize_rowwise_int4(W)\n",
    "\n",
    "          torch.cuda.synchronize()\n",
    "          t0 = time.time()\n",
    "          Y_q = matmul_fp16_int4(X, W_packed, scales, zeros)\n",
    "          torch.cuda.synchronize()\n",
    "          t_int4 = (time.time() - t0) * 1000\n",
    "\n",
    "          diff = (Y_fp16 - Y_q).abs()\n",
    "          mae = diff.mean().item()\n",
    "          maxe = diff.max().item()\n",
    "\n",
    "          print(f\"fp16:  {t_fp16:.3f} ms\")\n",
    "          print(f\"int4:  {t_int4:.3f} ms\")\n",
    "          print(f\"speed: Ã—{t_fp16 / t_int4:.3f}\")\n",
    "          print(f\"err:   MAE={mae:.4f}, MAX={maxe:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:39:34.998177Z",
     "iopub.status.busy": "2025-11-21T21:39:34.997830Z",
     "iopub.status.idle": "2025-11-21T21:39:36.306024Z",
     "shell.execute_reply": "2025-11-21T21:39:36.305207Z",
     "shell.execute_reply.started": "2025-11-21T21:39:34.998154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup...\n",
      "Warmup done.\n",
      "\n",
      "=== X: 128Ã—4096, W: 4096Ã—4096 ===\n",
      "fp16:  0.640 ms\n",
      "int4:  2.893 ms\n",
      "speed: Ã—0.221\n",
      "err:   MAE=7.1211, MAX=40.84\n",
      "\n",
      "=== X: 512Ã—4096, W: 4096Ã—4096 ===\n",
      "fp16:  1.046 ms\n",
      "int4:  8.905 ms\n",
      "speed: Ã—0.118\n",
      "err:   MAE=7.1289, MAX=46.56\n",
      "\n",
      "=== X: 2048Ã—4096, W: 4096Ã—4096 ===\n",
      "fp16:  3.469 ms\n",
      "int4:  35.033 ms\n",
      "speed: Ã—0.099\n",
      "err:   MAE=7.1250, MAX=50.88\n",
      "\n",
      "=== X: 128Ã—4096, W: 14336Ã—4096 ===\n",
      "fp16:  0.920 ms\n",
      "int4:  8.213 ms\n",
      "speed: Ã—0.112\n",
      "err:   MAE=7.1211, MAX=44.94\n",
      "\n",
      "=== X: 512Ã—4096, W: 14336Ã—4096 ===\n",
      "fp16:  3.095 ms\n",
      "int4:  30.299 ms\n",
      "speed: Ã—0.102\n",
      "err:   MAE=7.1211, MAX=50.94\n",
      "\n",
      "=== X: 2048Ã—4096, W: 14336Ã—4096 ===\n",
      "fp16:  11.533 ms\n",
      "int4:  107.047 ms\n",
      "speed: Ã—0.108\n",
      "err:   MAE=7.1289, MAX=52.12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark_matmuls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:39:36.306970Z",
     "iopub.status.busy": "2025-11-21T21:39:36.306769Z",
     "iopub.status.idle": "2025-11-21T21:39:36.315139Z",
     "shell.execute_reply": "2025-11-21T21:39:36.314375Z",
     "shell.execute_reply.started": "2025-11-21T21:39:36.306955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "def batchify_texts(texts, tokenizer, batch_size=8, max_tokens=128):\n",
    "    batch = []\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        enc = tokenizer(\n",
    "            t,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_tokens,\n",
    "        )\n",
    "\n",
    "        batch.append(enc[\"input_ids\"][0])\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            padded = torch.nn.utils.rnn.pad_sequence(\n",
    "                batch, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "            )\n",
    "            yield padded\n",
    "            batch = []\n",
    "\n",
    "    if batch:\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            batch, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "        )\n",
    "        yield padded\n",
    "\n",
    "def fast_ppl(model, tokenizer, batch_size=8, max_tokens=128, limit=None):\n",
    "    model.gradient_checkpointing_disable()\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")[\"text\"]\n",
    "    texts = ds if limit is None else ds[:limit]\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch in batchify_texts(texts, tokenizer, batch_size, max_tokens):\n",
    "        batch = batch.to(\"cuda\")\n",
    "\n",
    "        # shift\n",
    "        labels = batch[:, 1:].clone()\n",
    "        inputs = batch[:, :-1].clone()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=inputs).logits\n",
    "\n",
    "        # align shapes\n",
    "        logits = logits.reshape(-1, logits.size(-1))\n",
    "        labels = labels.reshape(-1)\n",
    "\n",
    "        mask = labels != tokenizer.pad_token_id\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits[mask],\n",
    "            labels[mask],\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item() * mask.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    ppl = math.exp(total_loss / total_tokens)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T21:39:36.316006Z",
     "iopub.status.busy": "2025-11-21T21:39:36.315764Z",
     "iopub.status.idle": "2025-11-21T21:42:47.859495Z",
     "shell.execute_reply": "2025-11-21T21:42:47.858746Z",
     "shell.execute_reply.started": "2025-11-21T21:39:36.315986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d622e704036647099f1cedea20a932a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d28511c1f349c8af2b12ee9c0c5eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(â€¦):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b63af4dfd104e9594c1ea9a8f4005e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(â€¦):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30ad9c955a542a8bd36abad77048611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(â€¦):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3d01ebd56a491eb999d158be360dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9d79f8dced47fab3cea2b49480d897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ed32d4d1c246c9ab920888e5dd5110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAST-PPL = 48.24247001214746\n"
     ]
    }
   ],
   "source": [
    "ppl = fast_ppl(\n",
    "    model, tokenizer,\n",
    "    batch_size=8,\n",
    "    max_tokens=128,\n",
    "    limit=5000,\n",
    ")\n",
    "\n",
    "print(\"FAST-PPL =\", ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, datetime, pathlib\n",
    "log_path = pathlib.Path('metrics_log.csv')\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "row = {\n",
    "    'timestamp': datetime.datetime.utcnow().isoformat(),\n",
    "    'model': model_name,\n",
    "    'quant': 'asym',\n",
    "    'targets': ';'.join(sorted(TARGETS)),\n",
    "    'ppl': float(ppl),\n",
    "    'batch_size': 8,\n",
    "    'max_tokens': 128,\n",
    "    'limit': 5000,\n",
    "}\n",
    "exists = log_path.exists()\n",
    "with log_path.open('a', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "    if not exists:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(row)\n",
    "print('Logged metrics to', log_path)\n",
    "print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
